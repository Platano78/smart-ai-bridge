# Smart AI Bridge - Environment Configuration Template
# Copy this file to .env and fill in your actual values

# ===================================
# LOCAL AI AUTO-DETECTION (v1.2.1+)
# ===================================

# Smart AI Bridge automatically discovers local LLM services (vLLM, Ollama, LM Studio)
# Priority: ENV override → Cached (5min) → Discovery → Expired cache fallback

# OPTIONAL: Force specific local endpoint (highest priority)
# LOCAL_AI_ENDPOINT=http://localhost:8002/v1

# OPTIONAL: Disable auto-detection (default: enabled)
# LOCAL_AI_DISCOVERY_ENABLED=false

# OPTIONAL: Adjust discovery cache TTL in milliseconds (default: 300000 = 5 min)
# LOCAL_AI_DISCOVERY_CACHE_TTL=600000

# Discovery scans these ports in order: 8002, 8001, 8000, 1234, 5000, 5001, 11434, 8080
# Validates endpoints contain actual LLM models (rejects generic APIs like Agent Genesis)

# ===================================
# REQUIRED: Cloud AI API Keys
# ===================================

# NVIDIA Cloud API Key (Recommended - Get from: https://build.nvidia.com/)
# Provides access to DeepSeek V3.1 and Qwen 3 Coder 480B
NVIDIA_API_KEY=nvapi-YOUR-KEY-HERE

# Google Gemini API Key (Optional - Get from: https://makersuite.google.com/app/apikey)
# Provides 2M token context window
GEMINI_API_KEY=YOUR-GEMINI-KEY-HERE

# ===================================
# SERVER CONFIGURATION
# ===================================

# Environment mode (production, development)
NODE_ENV=production

# Enable MCP server mode
MCP_SERVER_MODE=true

# Server port (default: 3000)
PORT=3000

# ===================================
# OPTIONAL: Local Model Configuration
# ===================================

# Local model endpoint (if running local LLM)
# DEEPSEEK_ENDPOINT=http://localhost:8001/v1

# Local model API key (optional security)
# LOCAL_MODEL_API_KEY=your-local-key

# ===================================
# OPTIONAL: Additional Cloud Providers
# ===================================

# DeepSeek Official API (if using direct DeepSeek cloud)
# DEEPSEEK_API_KEY=your-deepseek-key
# DEEPSEEK_ENDPOINT=https://api.deepseek.com/v1/chat/completions

# Qwen Cloud API (Alibaba DashScope)
# QWEN_CLOUD_API_KEY=your-qwen-key
# QWEN_CLOUD_ENDPOINT=https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation

# OpenAI API (fallback provider)
# OPENAI_API_KEY=your-openai-key

# ===================================
# ADVANCED CONFIGURATION
# ===================================

# Enable validation system
VALIDATION_ENABLED=true

# Enable fuzzy matching for file operations
FUZZY_MATCHING_ENABLED=true

# Fuzzy matching threshold (0.1-1.0, higher = more strict)
FUZZY_THRESHOLD=0.8

# Circuit breaker configuration
CIRCUIT_BREAKER_THRESHOLD=5
CIRCUIT_BREAKER_TIMEOUT=60000

# Rate limiting (requests per minute)
RATE_LIMIT_RPM=100

# ===================================
# SUBAGENT BACKEND CONFIGURATION
# ===================================

# Override which AI backend powers each subagent role
# Supported backends: local, gemini, deepseek3.1, qwen3, chatgpt, groq

# Role-specific overrides (optional - uncomment to use)
# SUBAGENT_CODE_REVIEWER_BACKEND=qwen3
# SUBAGENT_SECURITY_AUDITOR_BACKEND=deepseek3.1
# SUBAGENT_PLANNER_BACKEND=qwen3
# SUBAGENT_REFACTOR_SPECIALIST_BACKEND=deepseek3.1
# SUBAGENT_TEST_GENERATOR_BACKEND=deepseek3.1
# SUBAGENT_DOCUMENTATION_WRITER_BACKEND=gemini

# Global override for ALL subagents (takes priority over role-specific)
# SUBAGENT_DEFAULT_BACKEND=local

# MCP-Compliant Logging Configuration
# MCP protocol requires stdout ONLY for JSON-RPC messages
# All logging goes to stderr via console.error
# Options: silent, error, warn, info, debug
# Production: Use 'error' or 'warn' for minimal logging
# Development: Use 'info' or 'debug' for full diagnostics
MCP_LOG_LEVEL=info

# Legacy LOG_LEVEL (deprecated - use MCP_LOG_LEVEL instead)
# LOG_LEVEL=info

# ===================================
# AUTHENTICATION & SECURITY
# ===================================

# MCP Authentication Token (REQUIRED for production)
# Generate with: node -e "console.log(require('crypto').randomBytes(32).toString('hex'))"
MCP_AUTH_TOKEN=your-secure-token-here

# Tool-level permissions (comma-separated tool names, or * for all)
# Example: MCP_ALLOWED_TOOLS=read,review,write,edit
# MCP_ALLOWED_TOOLS=*

# Rate limiting configuration
# RATE_LIMIT_PER_MINUTE=60
# RATE_LIMIT_PER_HOUR=500
# RATE_LIMIT_PER_DAY=5000

# Payload size limits (in bytes)
# MAX_REQUEST_SIZE=10485760    # 10MB
# MAX_FILE_SIZE=5242880        # 5MB
# MAX_BATCH_SIZE=50            # Max files per batch operation

# ===================================
# IMPORTANT SECURITY NOTES
# ===================================

# Never commit this file with real values to version control!
# Add .env to .gitignore
# Set restrictive permissions: chmod 600 .env
# Always enable MCP_AUTH_TOKEN in production environments
# Development mode (no token) allows all operations - use only locally